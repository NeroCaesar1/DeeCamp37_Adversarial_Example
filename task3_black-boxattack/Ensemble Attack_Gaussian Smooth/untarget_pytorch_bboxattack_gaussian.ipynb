{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 首先将图片读入\n",
    "# 由于要发送json，所以需要对byte进行str解码\n",
    "def getByte(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img_byte = base64.b64encode(f.read())\n",
    "    img_str = img_byte.decode('ascii')\n",
    "    return img_str\n",
    "\n",
    "def call_api(path):\n",
    "    start_time = time.time()\n",
    "    # for i in tqdm(range(1000)):\n",
    "    img_str = getByte(path)\n",
    "    url = 'http://106.75.246.33:13501/face/face_api'\n",
    "    data = {'image': img_str, 'name':'100.jpg',\n",
    "            'type': '0', 'useAntiSpoofing': '0'}\n",
    "    json_mod = json.dumps(data)\n",
    "    res = requests.post(url=url, data=json_mod)\n",
    "    print(res.text)\n",
    "    end_time = time.time()\n",
    "    print('duration:',end_time-start_time,'s')\n",
    "    print('onetime',(end_time-start_time)/1000,'s')\n",
    "    return res.text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2tensor(img):\n",
    "    img = img.swapaxes(1, 2).swapaxes(0, 1)\n",
    "    img = np.reshape(img, [1, 3, 112, 112])\n",
    "    img = np.array(img, dtype = np.float32)    \n",
    "    img = (img - 127.5) / 128.0    \n",
    "    img = torch.from_numpy(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def gkern(kernlen=21, nsig=3):\n",
    "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
    "    import scipy.stats as st\n",
    "\n",
    "    interval = (2*nsig+1.)/(kernlen)\n",
    "    x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n",
    "    kern1d = np.diff(st.norm.cdf(x))\n",
    "    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n",
    "    kernel = kernel_raw/kernel_raw.sum()\n",
    "    return kernel\n",
    "\n",
    "\n",
    "class GaussianBlur(nn.Module):\n",
    "    def __init__(self, kernel):\n",
    "        super(GaussianBlur, self).__init__()\n",
    "        self.kernel_size = len(kernel)\n",
    "        print('kernel size is {0}.'.format(self.kernel_size))\n",
    "        assert self.kernel_size % 2 == 1, 'kernel size must be odd.'\n",
    "        \n",
    "        self.kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "        self.weight = nn.Parameter(data=self.kernel, requires_grad=False)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0,:,:].unsqueeze_(1)\n",
    "        x2 = x[:,1,:,:].unsqueeze_(1)\n",
    "        x3 = x[:,2,:,:].unsqueeze_(1)\n",
    "        padding = self.kernel_size // 2\n",
    "        x1 = F.conv2d(x1, self.weight, padding=padding)\n",
    "        x2 = F.conv2d(x2, self.weight, padding=padding)\n",
    "        x3 = F.conv2d(x3, self.weight, padding=padding)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Variation loss 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=1):\n",
    "        super(TVLoss,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self._tensor_size(x[:,:,1:,:])\n",
    "        count_w = self._tensor_size(x[:,:,:,1:])\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "\n",
    "    def _tensor_size(self,t):\n",
    "        return t.size()[1]*t.size()[2]*t.size()[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成多模型求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import scipy.stats as st\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "from backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\n",
    "from backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\n",
    "from backbone.model_facenet import model_920, model_921\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "#  ensemble multi-model\n",
    "# 1\n",
    "model_ir50_epoch120 = IR_50([112,112])\n",
    "model_ir50_epoch120.load_state_dict(torch.load('./Defense_Model/backbone_ir50_ms1m_epoch120.pth',map_location='cuda'))\n",
    "model_ir50_epoch120.eval()\n",
    "criterion_ir50_epoch120 = nn.MSELoss()\n",
    "# 2\n",
    "model_IR_152_Epoch_112 = IR_152([112,112])\n",
    "model_IR_152_Epoch_112.load_state_dict(torch.load('./Defense_Model/Backbone_IR_152_Epoch_112_Batch_2547328_Time_2019-07-13-02-59_checkpoint.pth',map_location='cuda'))\n",
    "model_IR_152_Epoch_112.eval()\n",
    "criterion_IR_152_Epoch_112 = nn.MSELoss()\n",
    "# 3\n",
    "model_IR_SE_50_Epoch_2 = IR_SE_50([112,112])\n",
    "model_IR_SE_50_Epoch_2.load_state_dict(torch.load('./Defense_Model/Backbone_IR_SE_50_Epoch_2_Batch_45488_Time_2019-08-03-19-39_checkpoint.pth',map_location='cuda'))\n",
    "model_IR_SE_50_Epoch_2.eval()\n",
    "criterion_IR_SE_50_Epoch_2 = nn.MSELoss()\n",
    "# 4\n",
    "model_IR_SE_152_Epoch_4 = IR_SE_152([112,112])\n",
    "model_IR_SE_152_Epoch_4.load_state_dict(torch.load('./Defense_Model/Backbone_IR_SE_152_Epoch_4_Batch_181956_Time_2019-08-06-07-29_checkpoint.pth',map_location='cuda'))\n",
    "model_IR_SE_152_Epoch_4.eval()\n",
    "criterion_IR_SE_152_Epoch_4 = nn.MSELoss()\n",
    "# 5\n",
    "model_ResNet_101_Epoch_4 = ResNet_101([112,112])\n",
    "model_ResNet_101_Epoch_4.load_state_dict(torch.load('./Defense_Model/Backbone_ResNet_101_Epoch_4_Batch_90976_Time_2019-08-04-11-34_checkpoint.pth',map_location='cuda'))\n",
    "model_ResNet_101_Epoch_4.eval()\n",
    "criterion_ResNet_101_Epoch_4 = nn.MSELoss()\n",
    "# 6\n",
    "model_ResNet_152_Epoch_1 = ResNet_152([112,112])\n",
    "model_ResNet_152_Epoch_1.load_state_dict(torch.load('./Defense_Model/Backbone_ResNet_152_Epoch_1_Batch_22744_Time_2019-08-03-01-01_checkpoint.pth',map_location='cuda'))\n",
    "model_ResNet_152_Epoch_1.eval()\n",
    "criterion_ResNet_152_Epoch_1 = nn.MSELoss()\n",
    "# 7\n",
    "model_ResNet_50_Epoch_3 = ResNet_50([112,112])\n",
    "model_ResNet_50_Epoch_3.load_state_dict(torch.load('./Defense_Model/Backbone_ResNet_50_Epoch_3_Batch_34116_Time_2019-08-02-19-12_checkpoint.pth',map_location='cuda'))\n",
    "model_ResNet_50_Epoch_3.eval()\n",
    "criterion_ResNet_50_Epoch_3 = nn.MSELoss()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# cpu \n",
    "# collect all images to attack\n",
    "paths = []\n",
    "picpath = os.getcwd()  + '/raw_faces'\n",
    "dire = None\n",
    "for root, dirs, files in os.walk(picpath): \n",
    "    if dirs and dirs != ['.ipynb_checkpoints']:\n",
    "        dire = dirs\n",
    "    for f in files:\n",
    "        paths.append(os.path.join(root, f))\n",
    "random.shuffle (paths)\n",
    "\n",
    "# paras\n",
    "eps = 1\n",
    "steps = 20 \n",
    "output_path = './output_img'\n",
    "momentum = 1.0\n",
    "\n",
    "        \n",
    "for path in paths:\n",
    "\n",
    "    start = time.time()   \n",
    "    print('processing ' + path + '  ===============>')\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # define paras\n",
    "    # in_tensor is origin tensor of image\n",
    "    # in_variable changes with gradient\n",
    "    in_tensor = img2tensor(np.array(image))\n",
    "    print(in_tensor.shape)\n",
    "    in_variable = in_tensor.detach()\n",
    "    in_tensor = in_tensor.squeeze()\n",
    "    adv = None\n",
    "\n",
    "\n",
    "    # origin feature \n",
    "    origin_feat_ir50_epoch120 = model_ir50_epoch120(in_variable) \n",
    "    origin_IR_152_Epoch_112 = model_IR_152_Epoch_112(in_variable) \n",
    "    origin_IR_SE_50_Epoch_2 = model_IR_SE_50_Epoch_2(in_variable) \n",
    "    origin_IR_SE_152_Epoch_4 = model_IR_SE_152_Epoch_4(in_variable) \n",
    "    origin_ResNet_101_Epoch_4 = model_ResNet_101_Epoch_4(in_variable) \n",
    "    origin_ResNet_152_Epoch_1 = model_ResNet_152_Epoch_1(in_variable) \n",
    "    origin_ResNet_50_Epoch_3 = model_ResNet_50_Epoch_3(in_variable) \n",
    "\n",
    "\n",
    "\n",
    "    # 1. untarget attack -> random noise\n",
    "    # 2. target attack -> x = alpha * target + (1 - alpha) * x\n",
    "    perturbation = torch.Tensor(3, 112, 112).uniform_(-0.1, 0.1)\n",
    "    in_variable += perturbation\n",
    "    in_variable.data.clamp_(-1.0, 1.0)\n",
    "    in_variable.requires_grad = True\n",
    "    g_noise = 0.0\n",
    "\n",
    "\n",
    "    #  sum gradient\n",
    "    for i in range(steps):\n",
    "            print('step: '+str(i))\n",
    "            #in_variable = in_variable.to(device)        \n",
    "            out_feat_ir50_epoch120 = model_ir50_epoch120(in_variable)\n",
    "            out_IR_152_Epoch_112 = model_IR_152_Epoch_112(in_variable)\n",
    "            out_IR_SE_50_Epoch_2 = model_IR_SE_50_Epoch_2(in_variable)\n",
    "            out_IR_SE_152_Epoch_4 = model_IR_SE_152_Epoch_4(in_variable)\n",
    "            out_ResNet_101_Epoch_4 = model_ResNet_101_Epoch_4(in_variable)\n",
    "            out_ResNet_152_Epoch_1 = model_ResNet_152_Epoch_1(in_variable)\n",
    "            out_ResNet_50_Epoch_3 = model_ResNet_50_Epoch_3(in_variable)\n",
    "\n",
    "            #loss = criterion(origin_feat_ir50_epoch120, out_feat_ir50_epoch120)  \n",
    "            loss = criterion(origin_feat_ir50_epoch120, out_feat_ir50_epoch120) + criterion(origin_IR_152_Epoch_112, out_IR_152_Epoch_112) + criterion(origin_IR_SE_50_Epoch_2 , out_IR_SE_50_Epoch_2 ) + criterion(origin_IR_SE_152_Epoch_4, out_IR_SE_152_Epoch_4)+ criterion(origin_ResNet_101_Epoch_4, out_ResNet_101_Epoch_4) + criterion(origin_ResNet_152_Epoch_1, out_ResNet_152_Epoch_1) + criterion(origin_ResNet_50_Epoch_3, out_ResNet_50_Epoch_3)\n",
    "\n",
    "\n",
    "            print('loss : %f' %loss)\n",
    "            # compute gradients\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            g_noise = momentum * g_noise + (in_variable.grad / in_variable.grad.data.norm(1))\n",
    "            g_noise = g_noise / g_noise.data.norm(1)\n",
    "            \n",
    "            if i % 2 == 0 :\n",
    "                kernel = gkern(3, 2).astype(np.float32)\n",
    "                gaussian_blur1 = GaussianBlur(kernel)\n",
    "                g_noise = gaussian_blur1(g_noise)\n",
    "                g_noise = torch.clamp(g_noise, -0.1, 0.1)\n",
    "            else:\n",
    "                addition = TVLoss()\n",
    "                g_noise = addition(g_noise)\n",
    "\n",
    "            in_variable.data = in_variable.data + ((eps/255.) * torch.sign(g_noise)) # * torch.from_numpy(mat).unsqueeze(0).float()\n",
    "\n",
    "            in_variable.grad.data.zero_() # unnecessary\n",
    "\n",
    "    # deprocess image\n",
    "    adv = in_variable.data.cpu().numpy()[0] # (3, 112, 112)   \n",
    "    perturbation = (adv - in_tensor.numpy())\n",
    "\n",
    "    adv = adv*128.0 + 127.0\n",
    "    adv = adv.swapaxes(0,1).swapaxes(1,2)\n",
    "    adv = adv[...,::-1]\n",
    "    adv = np.clip(adv, 0, 255).astype(np.uint8)\n",
    "\n",
    "    id = 100\n",
    "    advimg = './output_img/' + path.split('/')[-1].split('.')[-2] + '_' + str(id) + '.jpg'\n",
    "    id += 1\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imwrite(advimg, adv) \n",
    "    call_api(advimg)\n",
    "    print(\"save path is \" + advimg)\n",
    "    print('cost time is %.2f秒 ' %(time.time() - start)  )\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTHON3 root",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
